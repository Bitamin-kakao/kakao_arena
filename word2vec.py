# -*- coding: utf-8 -*-
"""word2vec.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Kekgn711BcAV0bMcEUMSMr5T7dlCBfc1
"""

#불용어 제거
def remove_stop_words(corpus):
    stop_words = ['is', 'a', 'will', 'be']
    results = []
    for text in corpus:
        tmp = text.split(' ')
        for stop_word in stop_words:
            if stop_word in tmp:
                tmp.remove(stop_word)
        results.append(" ".join(tmp))
    
    return results

corpus = remove_stop_words(corpus)
corpus

"""### word2vec table 생성"""

import pandas as pd
from nltk.tokenize import word_tokenize, sent_tokenize #단어토큰화
from gensim.models import Word2Vec #word2vec
from gensim.models import KeyedVectors
# for text in corpus:
#     print(text)

df = pd.DataFrame(data, columns = ['input', 'label'])
result = [word_tokenize(text) for text in corpus]

#size=w1,w2...의 개수
#window는 주변 몇개의 단어까지 고려할 것인가
model = Word2Vec(sentences=result, size=2, window=2, min_count=1, workers=4, sg=1)
model_result = model.wv.most_similar("man")
print(model_result)

#모델저장하기
from gensim.models import KeyedVectors
model.wv.save_word2vec_format('eng_w2v') # 모델 저장
loaded_model = KeyedVectors.load_word2vec_format("eng_w2v") #불러오기

#모델불러오기
model_result = loaded_model.most_similar("man")
print(model_result)

"""### kakao arena에 해보기

> #### 모듈 불러오기/설치
 - khaiii는 우분투 환경에서만 가능
 - 사전 추가할 경우에는 khaiii 다시 build 해야함
    - 다시 build하면 시간이 오래 걸려서 한번 추가할 때 몰아서 하는게 효율적일 듯
"""

import pickle
import pandas as pd
import sys
import re
from gensim.models import Word2Vec

#khaiii 설치
!git clone https://github.com/kakao/khaiii.git

!pip install cmake

!mkdir build

!cd build && cmake /content/khaiii

!cd /content/build/ && make all

!cd /content/build/ && make resource

!cd /content/build && make install

!cd /content/build && make package_python

!pip install /content/build/package_python

from khaiii import KhaiiiApi
api = KhaiiiApi()

"""> ### tag & playlist 같이 있는 데이터 만들기
- 한 행에 playlist 이름이랑 tag를 합쳤음
    - 이때 특수문자나 자음, 모음만 있는거는 모두 날림.
    - ex) '짝사랑 고백 사랑 이별 슬픔 감성을 자극하는 곡들 짝사랑 취향저격 슬픔 고백 사랑 이별'
- output.txt로 저장시킴
"""

# tag & playlist 하나로 합치고, cleansing 해서 txt파일로 저장
with open('train_genre.pickle', 'rb') as f:
    train = pickle.load(f)

def clean_str(text):
    pattern = '([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+)' # E-mail제거
    text = re.sub(pattern=pattern, repl='', string=text)
    pattern = '(http|ftp|https)://(?:[-\w.]|(?:%[\da-fA-F]{2}))+' # URL제거
    text = re.sub(pattern=pattern, repl='', string=text)
    pattern = '([ㄱ-ㅎㅏ-ㅣ]+)'  # 한글 자음, 모음 제거
    text = re.sub(pattern=pattern, repl='', string=text)
    pattern = '<[^>]*>'         # HTML 태그 제거
    text = re.sub(pattern=pattern, repl='', string=text)
    pattern = '[^\w\s]'         # 특수기호제거
    text = re.sub(pattern=pattern, repl='', string=text)
    return text  

i=0
import sys
text = open('output.txt','w')

while i < len(train):
    mylist = [[train.plylst_title[i]],train.tags[i],train.genre[i]]
    clean = clean_str(' '.join(sum(mylist,[])))
    text.write(clean+'\n')
    print(clean)
    i+=1
text.close()

with open('output.txt') as f:
    data = f.read().splitlines()

# ex = '그의 화려한 화려하는 바보이다'
# re.sub(r"(의|한|하는|이다)\b|그의", "",ex)

#원하는 파일/디렉토리 명 찾고싶을 때
!find / -name 'char_align.map*'

"""> ### khaiii 사전추가
- 사전추가 할 때 다시 build 하는 코드
"""

#사전추가
#!cd /content/khaiii/rsc # 어디에 있던 rsc로 와서 실행하기
#!mkdir -p ../build/share/khaiii
#!PYTHONPATH=/content/khaiii/src/main/python/ /content/khaiii/rsc/bin/compile_preanal.py --rsc-src=/content/khaiii/rsc/src --rsc-dir=/content/build/share/khaiii

!cd /content/build/ && make resource

!cd /content/build && make install

!cd /content/build && make package_python

!pip install /content/build/package_python

#사전추가 잘 됐는지 확인용(캐럴)
from khaiii import KhaiiiApi
api = KhaiiiApi()
for word in api.analyze(data[3]):
    print(word)

"""> ### khaiii 형태소 추출
 - 일단 명사만 추출하여 morphs에 저장
 - 이중리스트 형태
    - 제일 하부 리스트는 한 플레이리스트에 대한 명사임.
"""

#띄어쓰기 안되어있어서 이거만 일단 수작업으로 바꿔둠
data[6] = '짝사랑 고백 사랑 이별 슬픔 감성을 자극하는 곡들 짝사랑 취향저격 슬픔 고백 사랑 이별'

#토큰화하기
morphs = []
tmp_list = []
for i in range(0,len(data)):
    for word in api.analyze(data[i]):
        for morph in word.morphs:
            if morph.tag in ['NNG', 'NNP','NP']:
                tmp_list.append(morph.lex)
    morphs.append(tmp_list)
    tmp_list = []

"""> ### word2vec
- 내 생각: 한 자리 단어, 비슷한 단어 등을 묶어 주고 다시 해봐야할 듯. 지금 단어를 넣으면 비슷한 말들을 뽑아줘서 정작 tag를 안보여줌.
    - ex) '요즘'이랑 비슷한 단어 검색하면 '자주', '요새'가 나옴. 이건 너무 당연한 말이라서 분석이라고 보기가 힘듦. 이런 단어들 전처리 필요

- 저번에 블로그에서 봤던거 처럼 가중치 행렬을 만들어서 문장을 넣어도 태그를 예측할 수 있게끔 해도 좋을듯.
https://ratsgo.github.io/natural%20language%20processing/2017/03/08/word2vec/
"""

# word2vec with playlist_title
from gensim.models import Word2Vec
model_play = Word2Vec(sentences=morphs,
                 size=256, window=4,
                 min_count=2,
                 workers=4,
                 sg=1)
print(model_play)

#model_play.save('play_w2v.model') # 모델 저장

model_result = model_play.wv.most_similar("요즘")
print(model_result)

train.head(13)

model_play.wv.vectors # 가중치 값 보기

word_vectors = model_play.wv
vocabs = word_vectors.vocab.keys()
word_vectors_list = [word_vectors[v] for v in vocabs] # word2vec에 들어있는 단어 보여줌

# 두 단어의 거리(비슷한 정도) 계산
print(word_vectors.similarity(w1='새벽',w2='사랑'))